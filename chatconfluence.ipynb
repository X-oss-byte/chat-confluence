{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "368d525b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in ./chat_confluence.venv/lib/python3.11/site-packages (4.9.2)\n",
      "Requirement already satisfied: pytesseract in ./chat_confluence.venv/lib/python3.11/site-packages (0.3.10)\n",
      "Requirement already satisfied: Pillow in ./chat_confluence.venv/lib/python3.11/site-packages (9.5.0)\n",
      "Requirement already satisfied: packaging>=21.3 in ./chat_confluence.venv/lib/python3.11/site-packages (from pytesseract) (23.1)\n",
      "Requirement already satisfied: atlassian-python-api in ./chat_confluence.venv/lib/python3.11/site-packages (3.38.0)\n",
      "Requirement already satisfied: deprecated in ./chat_confluence.venv/lib/python3.11/site-packages (from atlassian-python-api) (1.2.14)\n",
      "Requirement already satisfied: requests in ./chat_confluence.venv/lib/python3.11/site-packages (from atlassian-python-api) (2.31.0)\n",
      "Requirement already satisfied: six in ./chat_confluence.venv/lib/python3.11/site-packages (from atlassian-python-api) (1.16.0)\n",
      "Requirement already satisfied: oauthlib in ./chat_confluence.venv/lib/python3.11/site-packages (from atlassian-python-api) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib in ./chat_confluence.venv/lib/python3.11/site-packages (from atlassian-python-api) (1.3.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in ./chat_confluence.venv/lib/python3.11/site-packages (from deprecated->atlassian-python-api) (1.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./chat_confluence.venv/lib/python3.11/site-packages (from requests->atlassian-python-api) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./chat_confluence.venv/lib/python3.11/site-packages (from requests->atlassian-python-api) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./chat_confluence.venv/lib/python3.11/site-packages (from requests->atlassian-python-api) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./chat_confluence.venv/lib/python3.11/site-packages (from requests->atlassian-python-api) (2023.5.7)\n",
      "Requirement already satisfied: langchain in ./chat_confluence.venv/lib/python3.11/site-packages (0.0.198)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in ./chat_confluence.venv/lib/python3.11/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./chat_confluence.venv/lib/python3.11/site-packages (from langchain) (2.0.15)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./chat_confluence.venv/lib/python3.11/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in ./chat_confluence.venv/lib/python3.11/site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: langchainplus-sdk>=0.0.7 in ./chat_confluence.venv/lib/python3.11/site-packages (from langchain) (0.0.8)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in ./chat_confluence.venv/lib/python3.11/site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./chat_confluence.venv/lib/python3.11/site-packages (from langchain) (1.24.2)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in ./chat_confluence.venv/lib/python3.11/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in ./chat_confluence.venv/lib/python3.11/site-packages (from langchain) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in ./chat_confluence.venv/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./chat_confluence.venv/lib/python3.11/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./chat_confluence.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./chat_confluence.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./chat_confluence.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./chat_confluence.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./chat_confluence.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./chat_confluence.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./chat_confluence.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in ./chat_confluence.venv/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in ./chat_confluence.venv/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in ./chat_confluence.venv/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./chat_confluence.venv/lib/python3.11/site-packages (from pydantic<2,>=1->langchain) (4.6.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./chat_confluence.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./chat_confluence.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./chat_confluence.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./chat_confluence.venv/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in ./chat_confluence.venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./chat_confluence.venv/lib/python3.11/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: tokenizers in ./chat_confluence.venv/lib/python3.11/site-packages (0.13.3)\n",
      "Requirement already satisfied: redis in ./chat_confluence.venv/lib/python3.11/site-packages (4.5.4)\n",
      "Requirement already satisfied: async-timeout>=4.0.2 in ./chat_confluence.venv/lib/python3.11/site-packages (from redis) (4.0.2)\n",
      "Requirement already satisfied: python-dotenv in ./chat_confluence.venv/lib/python3.11/site-packages (1.0.0)\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.4.0-cp311-cp311-macosx_10_9_x86_64.whl (797 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2023.6.3-cp311-cp311-macosx_10_9_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.7/294.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in ./chat_confluence.venv/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./chat_confluence.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./chat_confluence.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./chat_confluence.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./chat_confluence.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2023.6.3 tiktoken-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml\n",
    "!pip install pytesseract Pillow\n",
    "!pip install atlassian-python-api\n",
    "!pip install langchain\n",
    "!pip install tokenizers\n",
    "!pip install redis\n",
    "!pip install python-dotenv\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b2914e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PKNjrgMy4XvOJwD0VUapSU61qDf0oTfoX3Obnq\n",
      "current token is ODM5ODU5NDkyNDM5OuUkqSppg/23bPIO2PKiLrEcYfio\n"
     ]
    }
   ],
   "source": [
    "from confluence import ConfluenceLoader, ContentFormat\n",
    "import lxml\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "api_key_str = os.environ['CONFLUENCE_API_KEY']\n",
    "print(api_key_str)\n",
    "token_str = os.environ['CONFLUENCE_TOKEN']\n",
    "print(f\"current token is {token_str}\")\n",
    "\n",
    "loader = ConfluenceLoader(\n",
    "    url = \"https://confluence.zhenguanyu.com\",\n",
    "    cloud = False,\n",
    "    token = token_str\n",
    ")\n",
    "page_id = 192873905\n",
    "raw_cql = f\"type = page AND ancestor = {page_id}\"\n",
    "documents = loader.load(space_key=\"bigdataDoc\", include_attachments=False, content_format=ContentFormat.VIEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c89bbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n",
      "lc_kwargs={'page_content': '一、产品介绍 Sonic 计算平台提供 低延迟、高吞吐、高准确性 的数据处理能力。 支持通过Flink、Spark等多种计算引擎创建 实时、离线 部署，可通过 SQL 语句、 JAR 包等方式，实现复杂业务逻辑数据处理，简单易用，为用户提供计算作业的全生命周期管理。 二、产品优势 Sonic计算平台有以下主要功能： 打通多个智能云存储数据，包括：Kafka，Hive，ES等 支持SQL、JAR包等多种方式任务开发 实时、离线数据查询功能 计算任务全面监控，全周期管理功能 部署版本管理，且可进行版本恢复 元数据管理功能 支持用户自定义函数 三、应用场景 核心指标实时/离线报表：多维度实时/离线监测PV/UV、销量、销售额、续报率等走势 个性化推荐：根据用户的访问内容，为用户推荐个性化内容 直播课实时监控：通过对日志文件进行实时分析，及时发现视频中顿率、延迟、丢包等问题 详情说明，请参考： 有任何的需求和意见可以联系 基础研发部-大数据平台 @gaoyangbj @zhangnanbj @wangpengbj', 'metadata': {'title': 'Sonic 计算平台', 'id': '81453268', 'source': 'https://confluence.zhenguanyu.com/pages/viewpage.action?pageId=81453268'}} page_content='一、产品介绍 Sonic 计算平台提供 低延迟、高吞吐、高准确性 的数据处理能力。 支持通过Flink、Spark等多种计算引擎创建 实时、离线 部署，可通过 SQL 语句、 JAR 包等方式，实现复杂业务逻辑数据处理，简单易用，为用户提供计算作业的全生命周期管理。 二、产品优势 Sonic计算平台有以下主要功能： 打通多个智能云存储数据，包括：Kafka，Hive，ES等 支持SQL、JAR包等多种方式任务开发 实时、离线数据查询功能 计算任务全面监控，全周期管理功能 部署版本管理，且可进行版本恢复 元数据管理功能 支持用户自定义函数 三、应用场景 核心指标实时/离线报表：多维度实时/离线监测PV/UV、销量、销售额、续报率等走势 个性化推荐：根据用户的访问内容，为用户推荐个性化内容 直播课实时监控：通过对日志文件进行实时分析，及时发现视频中顿率、延迟、丢包等问题 详情说明，请参考： 有任何的需求和意见可以联系 基础研发部-大数据平台 @gaoyangbj @zhangnanbj @wangpengbj' metadata={'title': 'Sonic 计算平台', 'id': '81453268', 'source': 'https://confluence.zhenguanyu.com/pages/viewpage.action?pageId=81453268'}\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "398d4b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores.redis import Redis\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function = len)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6b45e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "embeddings = OpenAIEmbeddings(deployment=\"embeddings_model\", chunk_size=1)\n",
    "db = Redis.from_documents(docs, embeddings, redis_url=\"redis://10.134.5.70:6379\", index_name='chat_confluence_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b33c6bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain, _get_chat_history\n",
    "from typing import Dict, Any, Optional, List\n",
    "from langchain.callbacks.manager import CallbackManagerForChainRun\n",
    "from langchain.schema import Document\n",
    "\n",
    "class CustomConversationalRetrievalChain(ConversationalRetrievalChain):\n",
    "    last_source_documents: List[Document] = None\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.last_source_documents = None\n",
    "        \n",
    "    def _call(self, inputs: Dict[str, Any], run_manager: Optional[CallbackManagerForChainRun]=None) -> Dict[str, Any]:\n",
    "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
    "        question = inputs['question']\n",
    "        get_chat_history = self.get_chat_history or _get_chat_history\n",
    "        chat_history_str = get_chat_history(inputs['chat_history'])\n",
    "        if chat_history_str:\n",
    "            callbacks = _run_manager.get_child()\n",
    "            new_question = self.question_generator.run(question=question, chat_history=chat_history_str, callbacks=callbacks)\n",
    "        else:\n",
    "            new_question = question\n",
    "        docs = self._get_docs(new_question, inputs)\n",
    "        new_inputs = inputs.copy()\n",
    "        new_inputs['question'] = new_question\n",
    "        new_inputs['chat_history'] = chat_history_str\n",
    "        answer = self.combine_docs_chain.run(input_documents=docs, callbacks=_run_manager.get_child(), **new_inputs)\n",
    "        self.last_source_documents = docs\n",
    "        return {self.output_key: answer}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "def801dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"How to create a service?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain > 2:RunTypeEnum.chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain > 2:RunTypeEnum.chain:StuffDocumentsChain > 3:RunTypeEnum.chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"How to create a service?\",\n",
      "  \"context\": \"1 如何在 Console 上新建服务，需要提供什么信息 点击Console右上角的oncall，提oncall处理。oncall需要填写的信息如下： 系统：Console 类型：问题反馈类 概要：新建服务 gerrit项目名：你的项目名，如infra-fake-project 服务名：你的服务名，如：infra-fake-service 描述： 服务类型：http/rpc，静态服务，xxl-job服务，三选一 服务端口：如8080 如下图： 提交oncall后，会有专人处理（一般10min之内会处理）。 2 如何申请服务所需的机器资源 新建服务后，会在『服务列表』中展示对应的服务。 如果是service平台部署，点击『 申请service平台服务工单 』；申请工单后， 找运维审核 。 如果是k8s 部署，点『 修改k8s服务环境信息 』，填写好对应信息后，保存即可， 无需审核 （k8s在部署时，会自动调度机器资源）。 如下图：\\n\\n1 如何在 Console 上新建服务，需要提供什么信息 点击Console右上角的oncall，提oncall处理。oncall需要填写的信息如下： 系统：Console 类型：问题反馈类 概要：新建服务 gerrit项目名：你的项目名，如infra-fake-project 服务名：你的服务名，如：infra-fake-service 描述： 服务类型：http/rpc，静态服务，xxl-job服务，三选一 服务端口：如8080 如下图： 提交oncall后，会有专人处理（一般10min之内会处理）。 2 如何申请服务所需的机器资源 新建服务后，会在『服务列表』中展示对应的服务。 如果是service平台部署，点击『 申请service平台服务工单 』；申请工单后， 找运维审核 。 如果是k8s 部署，点『 修改k8s服务环境信息 』，填写好对应信息后，保存即可， 无需审核 （k8s在部署时，会自动调度机器资源）。 如下图：\\n\\n1 如何在 Console 上新建服务，需要提供什么信息 点击Console右上角的oncall，提oncall处理。oncall需要填写的信息如下： 系统：Console 类型：问题反馈类 概要：新建服务 gerrit项目名：你的项目名，如infra-fake-project 服务名：你的服务名，如：infra-fake-service 描述： 服务类型：http/rpc，静态服务，xxl-job服务，三选一 服务端口：如8080 如下图： 提交oncall后，会有专人处理（一般10min之内会处理）。 2 如何申请服务所需的机器资源 新建服务后，会在『服务列表』中展示对应的服务。 如果是service平台部署，点击『 申请service平台服务工单 』；申请工单后， 找运维审核 。 如果是k8s 部署，点『 修改k8s服务环境信息 』，填写好对应信息后，保存即可， 无需审核 （k8s在部署时，会自动调度机器资源）。 如下图：\\n\\n1 如何在 Console 上新建服务，需要提供什么信息 点击Console右上角的oncall，提oncall处理。oncall需要填写的信息如下： 系统：Console 类型：问题反馈类 概要：新建服务 gerrit项目名：你的项目名，如infra-fake-project 服务名：你的服务名，如：infra-fake-service 描述： 服务类型：http/rpc，静态服务，xxl-job服务，三选一 服务端口：如8080 如下图： 提交oncall后，会有专人处理（一般10min之内会处理）。 2 如何申请服务所需的机器资源 新建服务后，会在『服务列表』中展示对应的服务。 如果是service平台部署，点击『 申请service平台服务工单 』；申请工单后， 找运维审核 。 如果是k8s 部署，点『 修改k8s服务环境信息 』，填写好对应信息后，保存即可， 无需审核 （k8s在部署时，会自动调度机器资源）。 如下图：\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain > 2:RunTypeEnum.chain:StuffDocumentsChain > 3:RunTypeEnum.chain:LLMChain > 4:RunTypeEnum.llm:AzureChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n1 如何在 Console 上新建服务，需要提供什么信息 点击Console右上角的oncall，提oncall处理。oncall需要填写的信息如下： 系统：Console 类型：问题反馈类 概要：新建服务 gerrit项目名：你的项目名，如infra-fake-project 服务名：你的服务名，如：infra-fake-service 描述： 服务类型：http/rpc，静态服务，xxl-job服务，三选一 服务端口：如8080 如下图： 提交oncall后，会有专人处理（一般10min之内会处理）。 2 如何申请服务所需的机器资源 新建服务后，会在『服务列表』中展示对应的服务。 如果是service平台部署，点击『 申请service平台服务工单 』；申请工单后， 找运维审核 。 如果是k8s 部署，点『 修改k8s服务环境信息 』，填写好对应信息后，保存即可， 无需审核 （k8s在部署时，会自动调度机器资源）。 如下图：\\n\\n1 如何在 Console 上新建服务，需要提供什么信息 点击Console右上角的oncall，提oncall处理。oncall需要填写的信息如下： 系统：Console 类型：问题反馈类 概要：新建服务 gerrit项目名：你的项目名，如infra-fake-project 服务名：你的服务名，如：infra-fake-service 描述： 服务类型：http/rpc，静态服务，xxl-job服务，三选一 服务端口：如8080 如下图： 提交oncall后，会有专人处理（一般10min之内会处理）。 2 如何申请服务所需的机器资源 新建服务后，会在『服务列表』中展示对应的服务。 如果是service平台部署，点击『 申请service平台服务工单 』；申请工单后， 找运维审核 。 如果是k8s 部署，点『 修改k8s服务环境信息 』，填写好对应信息后，保存即可， 无需审核 （k8s在部署时，会自动调度机器资源）。 如下图：\\n\\n1 如何在 Console 上新建服务，需要提供什么信息 点击Console右上角的oncall，提oncall处理。oncall需要填写的信息如下： 系统：Console 类型：问题反馈类 概要：新建服务 gerrit项目名：你的项目名，如infra-fake-project 服务名：你的服务名，如：infra-fake-service 描述： 服务类型：http/rpc，静态服务，xxl-job服务，三选一 服务端口：如8080 如下图： 提交oncall后，会有专人处理（一般10min之内会处理）。 2 如何申请服务所需的机器资源 新建服务后，会在『服务列表』中展示对应的服务。 如果是service平台部署，点击『 申请service平台服务工单 』；申请工单后， 找运维审核 。 如果是k8s 部署，点『 修改k8s服务环境信息 』，填写好对应信息后，保存即可， 无需审核 （k8s在部署时，会自动调度机器资源）。 如下图：\\n\\n1 如何在 Console 上新建服务，需要提供什么信息 点击Console右上角的oncall，提oncall处理。oncall需要填写的信息如下： 系统：Console 类型：问题反馈类 概要：新建服务 gerrit项目名：你的项目名，如infra-fake-project 服务名：你的服务名，如：infra-fake-service 描述： 服务类型：http/rpc，静态服务，xxl-job服务，三选一 服务端口：如8080 如下图： 提交oncall后，会有专人处理（一般10min之内会处理）。 2 如何申请服务所需的机器资源 新建服务后，会在『服务列表』中展示对应的服务。 如果是service平台部署，点击『 申请service平台服务工单 』；申请工单后， 找运维审核 。 如果是k8s 部署，点『 修改k8s服务环境信息 』，填写好对应信息后，保存即可， 无需审核 （k8s在部署时，会自动调度机器资源）。 如下图：\\nHuman: How to create a service?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain > 2:RunTypeEnum.chain:StuffDocumentsChain > 3:RunTypeEnum.chain:LLMChain > 4:RunTypeEnum.llm:AzureChatOpenAI] [6.26s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To create a service, you need to click on \\\"oncall\\\" in the upper right corner of the console and fill out the following information:\\n\\n- System: Console\\n- Type: Problem feedback\\n- Summary: New service\\n- Gerrit project name: Your project name, such as infra-fake-project\\n- Service name: Your service name, such as infra-fake-service\\n- Description:\\n- Service type: HTTP/RPC, static service, or xxl-job service\\n- Service port: such as 8080\\n\\nAfter submitting the oncall, a specialist will handle it within 10 minutes. Once the service is created, it will be displayed in the service list. If it's a service platform deployment, you can click \\\"apply for service platform service work order\\\" and then find an operator to review it. If it's a K8s deployment, you can click \\\"modify K8s service environment information,\\\" fill in the corresponding information, and save it without the need for review (K8s will automatically schedule machine resources during deployment).\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"To create a service, you need to click on \\\"oncall\\\" in the upper right corner of the console and fill out the following information:\\n\\n- System: Console\\n- Type: Problem feedback\\n- Summary: New service\\n- Gerrit project name: Your project name, such as infra-fake-project\\n- Service name: Your service name, such as infra-fake-service\\n- Description:\\n- Service type: HTTP/RPC, static service, or xxl-job service\\n- Service port: such as 8080\\n\\nAfter submitting the oncall, a specialist will handle it within 10 minutes. Once the service is created, it will be displayed in the service list. If it's a service platform deployment, you can click \\\"apply for service platform service work order\\\" and then find an operator to review it. If it's a K8s deployment, you can click \\\"modify K8s service environment information,\\\" fill in the corresponding information, and save it without the need for review (K8s will automatically schedule machine resources during deployment).\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 215,\n",
      "      \"prompt_tokens\": 1207,\n",
      "      \"total_tokens\": 1422\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain > 2:RunTypeEnum.chain:StuffDocumentsChain > 3:RunTypeEnum.chain:LLMChain] [6.26s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"To create a service, you need to click on \\\"oncall\\\" in the upper right corner of the console and fill out the following information:\\n\\n- System: Console\\n- Type: Problem feedback\\n- Summary: New service\\n- Gerrit project name: Your project name, such as infra-fake-project\\n- Service name: Your service name, such as infra-fake-service\\n- Description:\\n- Service type: HTTP/RPC, static service, or xxl-job service\\n- Service port: such as 8080\\n\\nAfter submitting the oncall, a specialist will handle it within 10 minutes. Once the service is created, it will be displayed in the service list. If it's a service platform deployment, you can click \\\"apply for service platform service work order\\\" and then find an operator to review it. If it's a K8s deployment, you can click \\\"modify K8s service environment information,\\\" fill in the corresponding information, and save it without the need for review (K8s will automatically schedule machine resources during deployment).\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain > 2:RunTypeEnum.chain:StuffDocumentsChain] [6.26s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"To create a service, you need to click on \\\"oncall\\\" in the upper right corner of the console and fill out the following information:\\n\\n- System: Console\\n- Type: Problem feedback\\n- Summary: New service\\n- Gerrit project name: Your project name, such as infra-fake-project\\n- Service name: Your service name, such as infra-fake-service\\n- Description:\\n- Service type: HTTP/RPC, static service, or xxl-job service\\n- Service port: such as 8080\\n\\nAfter submitting the oncall, a specialist will handle it within 10 minutes. Once the service is created, it will be displayed in the service list. If it's a service platform deployment, you can click \\\"apply for service platform service work order\\\" and then find an operator to review it. If it's a K8s deployment, you can click \\\"modify K8s service environment information,\\\" fill in the corresponding information, and save it without the need for review (K8s will automatically schedule machine resources during deployment).\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain] [6.92s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \"To create a service, you need to click on \\\"oncall\\\" in the upper right corner of the console and fill out the following information:\\n\\n- System: Console\\n- Type: Problem feedback\\n- Summary: New service\\n- Gerrit project name: Your project name, such as infra-fake-project\\n- Service name: Your service name, such as infra-fake-service\\n- Description:\\n- Service type: HTTP/RPC, static service, or xxl-job service\\n- Service port: such as 8080\\n\\nAfter submitting the oncall, a specialist will handle it within 10 minutes. Once the service is created, it will be displayed in the service list. If it's a service platform deployment, you can click \\\"apply for service platform service work order\\\" and then find an operator to review it. If it's a K8s deployment, you can click \\\"modify K8s service environment information,\\\" fill in the corresponding information, and save it without the need for review (K8s will automatically schedule machine resources during deployment).\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain > 2:RunTypeEnum.chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"\",\n",
      "  \"chat_history\": \"\\nHuman: How to create a service?\\nAssistant: To create a service, you need to click on \\\"oncall\\\" in the upper right corner of the console and fill out the following information:\\n\\n- System: Console\\n- Type: Problem feedback\\n- Summary: New service\\n- Gerrit project name: Your project name, such as infra-fake-project\\n- Service name: Your service name, such as infra-fake-service\\n- Description:\\n- Service type: HTTP/RPC, static service, or xxl-job service\\n- Service port: such as 8080\\n\\nAfter submitting the oncall, a specialist will handle it within 10 minutes. Once the service is created, it will be displayed in the service list. If it's a service platform deployment, you can click \\\"apply for service platform service work order\\\" and then find an operator to review it. If it's a K8s deployment, you can click \\\"modify K8s service environment information,\\\" fill in the corresponding information, and save it without the need for review (K8s will automatically schedule machine resources during deployment).\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain > 2:RunTypeEnum.chain:LLMChain > 3:RunTypeEnum.llm:AzureChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: How to create a service?\\nAssistant: To create a service, you need to click on \\\"oncall\\\" in the upper right corner of the console and fill out the following information:\\n\\n- System: Console\\n- Type: Problem feedback\\n- Summary: New service\\n- Gerrit project name: Your project name, such as infra-fake-project\\n- Service name: Your service name, such as infra-fake-service\\n- Description:\\n- Service type: HTTP/RPC, static service, or xxl-job service\\n- Service port: such as 8080\\n\\nAfter submitting the oncall, a specialist will handle it within 10 minutes. Once the service is created, it will be displayed in the service list. If it's a service platform deployment, you can click \\\"apply for service platform service work order\\\" and then find an operator to review it. If it's a K8s deployment, you can click \\\"modify K8s service environment information,\\\" fill in the corresponding information, and save it without the need for review (K8s will automatically schedule machine resources during deployment).\\nFollow Up Input: \\nStandalone question:\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain > 2:RunTypeEnum.chain:LLMChain > 3:RunTypeEnum.llm:AzureChatOpenAI] [976.818ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"What is the process for creating a service and how long does it take for a specialist to handle it?\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"What is the process for creating a service and how long does it take for a specialist to handle it?\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 21,\n",
      "      \"prompt_tokens\": 272,\n",
      "      \"total_tokens\": 293\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain > 2:RunTypeEnum.chain:LLMChain] [978.202ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"What is the process for creating a service and how long does it take for a specialist to handle it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain > 4:RunTypeEnum.chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain > 4:RunTypeEnum.chain:StuffDocumentsChain > 5:RunTypeEnum.chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What is the process for creating a service and how long does it take for a specialist to handle it?\",\n",
      "  \"context\": \"1 如何在 Console 上新建服务，需要提供什么信息 点击Console右上角的oncall，提oncall处理。oncall需要填写的信息如下： 系统：Console 类型：问题反馈类 概要：新建服务 gerrit项目名：你的项目名，如infra-fake-project 服务名：你的服务名，如：infra-fake-service 描述： 服务类型：http/rpc，静态服务，xxl-job服务，三选一 服务端口：如8080 如下图： 提交oncall后，会有专人处理（一般10min之内会处理）。 2 如何申请服务所需的机器资源 新建服务后，会在『服务列表』中展示对应的服务。 如果是service平台部署，点击『 申请service平台服务工单 』；申请工单后， 找运维审核 。 如果是k8s 部署，点『 修改k8s服务环境信息 』，填写好对应信息后，保存即可， 无需审核 （k8s在部署时，会自动调度机器资源）。 如下图：\\n\\n1 如何在 Console 上新建服务，需要提供什么信息 点击Console右上角的oncall，提oncall处理。oncall需要填写的信息如下： 系统：Console 类型：问题反馈类 概要：新建服务 gerrit项目名：你的项目名，如infra-fake-project 服务名：你的服务名，如：infra-fake-service 描述： 服务类型：http/rpc，静态服务，xxl-job服务，三选一 服务端口：如8080 如下图： 提交oncall后，会有专人处理（一般10min之内会处理）。 2 如何申请服务所需的机器资源 新建服务后，会在『服务列表』中展示对应的服务。 如果是service平台部署，点击『 申请service平台服务工单 』；申请工单后， 找运维审核 。 如果是k8s 部署，点『 修改k8s服务环境信息 』，填写好对应信息后，保存即可， 无需审核 （k8s在部署时，会自动调度机器资源）。 如下图：\\n\\n1 如何在 Console 上新建服务，需要提供什么信息 点击Console右上角的oncall，提oncall处理。oncall需要填写的信息如下： 系统：Console 类型：问题反馈类 概要：新建服务 gerrit项目名：你的项目名，如infra-fake-project 服务名：你的服务名，如：infra-fake-service 描述： 服务类型：http/rpc，静态服务，xxl-job服务，三选一 服务端口：如8080 如下图： 提交oncall后，会有专人处理（一般10min之内会处理）。 2 如何申请服务所需的机器资源 新建服务后，会在『服务列表』中展示对应的服务。 如果是service平台部署，点击『 申请service平台服务工单 』；申请工单后， 找运维审核 。 如果是k8s 部署，点『 修改k8s服务环境信息 』，填写好对应信息后，保存即可， 无需审核 （k8s在部署时，会自动调度机器资源）。 如下图：\\n\\n1 如何在 Console 上新建服务，需要提供什么信息 点击Console右上角的oncall，提oncall处理。oncall需要填写的信息如下： 系统：Console 类型：问题反馈类 概要：新建服务 gerrit项目名：你的项目名，如infra-fake-project 服务名：你的服务名，如：infra-fake-service 描述： 服务类型：http/rpc，静态服务，xxl-job服务，三选一 服务端口：如8080 如下图： 提交oncall后，会有专人处理（一般10min之内会处理）。 2 如何申请服务所需的机器资源 新建服务后，会在『服务列表』中展示对应的服务。 如果是service平台部署，点击『 申请service平台服务工单 』；申请工单后， 找运维审核 。 如果是k8s 部署，点『 修改k8s服务环境信息 』，填写好对应信息后，保存即可， 无需审核 （k8s在部署时，会自动调度机器资源）。 如下图：\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain > 4:RunTypeEnum.chain:StuffDocumentsChain > 5:RunTypeEnum.chain:LLMChain > 6:RunTypeEnum.llm:AzureChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n1 如何在 Console 上新建服务，需要提供什么信息 点击Console右上角的oncall，提oncall处理。oncall需要填写的信息如下： 系统：Console 类型：问题反馈类 概要：新建服务 gerrit项目名：你的项目名，如infra-fake-project 服务名：你的服务名，如：infra-fake-service 描述： 服务类型：http/rpc，静态服务，xxl-job服务，三选一 服务端口：如8080 如下图： 提交oncall后，会有专人处理（一般10min之内会处理）。 2 如何申请服务所需的机器资源 新建服务后，会在『服务列表』中展示对应的服务。 如果是service平台部署，点击『 申请service平台服务工单 』；申请工单后， 找运维审核 。 如果是k8s 部署，点『 修改k8s服务环境信息 』，填写好对应信息后，保存即可， 无需审核 （k8s在部署时，会自动调度机器资源）。 如下图：\\n\\n1 如何在 Console 上新建服务，需要提供什么信息 点击Console右上角的oncall，提oncall处理。oncall需要填写的信息如下： 系统：Console 类型：问题反馈类 概要：新建服务 gerrit项目名：你的项目名，如infra-fake-project 服务名：你的服务名，如：infra-fake-service 描述： 服务类型：http/rpc，静态服务，xxl-job服务，三选一 服务端口：如8080 如下图： 提交oncall后，会有专人处理（一般10min之内会处理）。 2 如何申请服务所需的机器资源 新建服务后，会在『服务列表』中展示对应的服务。 如果是service平台部署，点击『 申请service平台服务工单 』；申请工单后， 找运维审核 。 如果是k8s 部署，点『 修改k8s服务环境信息 』，填写好对应信息后，保存即可， 无需审核 （k8s在部署时，会自动调度机器资源）。 如下图：\\n\\n1 如何在 Console 上新建服务，需要提供什么信息 点击Console右上角的oncall，提oncall处理。oncall需要填写的信息如下： 系统：Console 类型：问题反馈类 概要：新建服务 gerrit项目名：你的项目名，如infra-fake-project 服务名：你的服务名，如：infra-fake-service 描述： 服务类型：http/rpc，静态服务，xxl-job服务，三选一 服务端口：如8080 如下图： 提交oncall后，会有专人处理（一般10min之内会处理）。 2 如何申请服务所需的机器资源 新建服务后，会在『服务列表』中展示对应的服务。 如果是service平台部署，点击『 申请service平台服务工单 』；申请工单后， 找运维审核 。 如果是k8s 部署，点『 修改k8s服务环境信息 』，填写好对应信息后，保存即可， 无需审核 （k8s在部署时，会自动调度机器资源）。 如下图：\\n\\n1 如何在 Console 上新建服务，需要提供什么信息 点击Console右上角的oncall，提oncall处理。oncall需要填写的信息如下： 系统：Console 类型：问题反馈类 概要：新建服务 gerrit项目名：你的项目名，如infra-fake-project 服务名：你的服务名，如：infra-fake-service 描述： 服务类型：http/rpc，静态服务，xxl-job服务，三选一 服务端口：如8080 如下图： 提交oncall后，会有专人处理（一般10min之内会处理）。 2 如何申请服务所需的机器资源 新建服务后，会在『服务列表』中展示对应的服务。 如果是service平台部署，点击『 申请service平台服务工单 』；申请工单后， 找运维审核 。 如果是k8s 部署，点『 修改k8s服务环境信息 』，填写好对应信息后，保存即可， 无需审核 （k8s在部署时，会自动调度机器资源）。 如下图：\\nHuman: What is the process for creating a service and how long does it take for a specialist to handle it?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain > 4:RunTypeEnum.chain:StuffDocumentsChain > 5:RunTypeEnum.chain:LLMChain > 6:RunTypeEnum.llm:AzureChatOpenAI] [2.15s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To create a service on Console, you need to click oncall on the upper right corner of the page and provide the following information: system, type, summary, gerrit project name, service name, description, service type, and service port. After you submit the oncall, a specialist will handle it within 10 minutes.\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"To create a service on Console, you need to click oncall on the upper right corner of the page and provide the following information: system, type, summary, gerrit project name, service name, description, service type, and service port. After you submit the oncall, a specialist will handle it within 10 minutes.\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 68,\n",
      "      \"prompt_tokens\": 1222,\n",
      "      \"total_tokens\": 1290\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain > 4:RunTypeEnum.chain:StuffDocumentsChain > 5:RunTypeEnum.chain:LLMChain] [2.15s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"To create a service on Console, you need to click oncall on the upper right corner of the page and provide the following information: system, type, summary, gerrit project name, service name, description, service type, and service port. After you submit the oncall, a specialist will handle it within 10 minutes.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain > 4:RunTypeEnum.chain:StuffDocumentsChain] [2.15s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"To create a service on Console, you need to click oncall on the upper right corner of the page and provide the following information: system, type, summary, gerrit project name, service name, description, service type, and service port. After you submit the oncall, a specialist will handle it within 10 minutes.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:RunTypeEnum.chain:CustomConversationalRetrievalChain] [4.51s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \"To create a service on Console, you need to click oncall on the upper right corner of the page and provide the following information: system, type, summary, gerrit project name, service name, description, service type, and service port. After you submit the oncall, a specialist will handle it within 10 minutes.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores.redis import Redis\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import langchain\n",
    "import os\n",
    "embeddings = OpenAIEmbeddings(deployment=\"embeddings_model\", chunk_size=1)\n",
    "\n",
    "query_db = Redis.from_existing_index(embeddings, redis_url=\"redis://10.134.5.70:6379\", index_name='chat_confluence_new')\n",
    "retriever = query_db.as_retriever()\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "llm = AzureChatOpenAI(deployment_name=\"chat_confluence\",model_name=\"gpt-35-turbo\", verbose=True)\n",
    "qa = CustomConversationalRetrievalChain.from_llm(llm, retriever, memory=memory)\n",
    "langchain.debug=True\n",
    "res1 = qa({\"question\": \"How to create a service?\"})\n",
    "# print(f\"answer is {res1}, source_docs is {qa.last_source_documents}\")\n",
    "res2 = qa({\"question\": \"\"})\n",
    "# print(f\"res2 is {res2}, source_docs are {qa.last_source_documents}\")\n",
    "langchain.debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fd519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
